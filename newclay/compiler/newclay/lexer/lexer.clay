
import newclay.common.*;
import newclay.diagnostics.*;
import maybe.*;
import parsing.combinators.wrapper.*;
import externals.*;



//
// data definitions
//

enum TokenKind {
    SENTINEL,
    PUNCTUATION,
    KEYWORD,
    IDENTIFIER,
    STRING_LITERAL,
    CHAR_LITERAL,
    INT_LITERAL,
    FLOAT_LITERAL,
    STATIC_INDEX,
    LITERAL_CODE,
    SPACE,
    COMMENT,
    INDENTATION,
    OPEN_BLOCK,
    CLOSE_BLOCK,
    END_STATEMENT,
}

alias DataRange = Range[SizeT];

record Token(kind:TokenKind, subKind:UInt8, range:DataRange);

[I | Integer?(I)]
overload Token(kind:TokenKind, subKind:I) =
    Token(kind, UInt8(subKind), DataRange(SizeT(0), SizeT(0)));

overload Token(kind:TokenKind) = Token(kind, 0);

record SourceTokens = referenceType(
    file: SourceFile,
    vector: Vector[Token],
);



//
// punctuations and keywords
//

// 34 punctuations
var punctuations = [
    "...", "<--",
    "=>", "->",
    "<=", ">=", "<", ">",
    "!=", "==",
    "+=", "-=", "*=", "/=",
    "..",
    ":",
    "(", ")", "[", "]",
    "|",
    "=",
    "'",
    ";", ",",
    "*", ".",
    "{", "}",
    "+", "-",
    "/",
    "&", "^",
    "#",
];

// 39 keywords
var keywords = [
    "import", "as",
    "external", "inline",
    "symbol", "attribute",
    "overload", "static",
    "var", "const", "ref", "rvalue", "forward", "alias",
    "true", "false",
    "if", "then", "else",
    "and", "or", "not",
    "public", "private",
    "return", "break", "continue",
    "try", "catch", "throw",
    "for", "in",
    "while",
    "switch", "case",
    "div", "mod",
    "__c__", "__llvm__", "__asm__",
];

punctuationIndex(s) Int {
    for (i, x in enumerated(punctuations)) {
        if (x == s)
            return Int(i);
    }
    return -1;
}

keywordIndex(s) Int {
    for (i, x in enumerated(keywords)) {
        if (x == s)
            return Int(i);
    }
    return -1;
}



//
// lexer input
//

record LexerInput(
    file : SourceFile,
    current : SizeT,
    maxCurrent : SizeT,
);

overload LexerInput(file : SourceFile) =
    LexerInput(file, SizeT(0), SizeT(0));

overload iterator(x:LexerInput) = x;

overload hasNext?(x:LexerInput) = (x.current < size(x.file.data));

overload next(x:LexerInput) {
    var c = x.file.data[x.current];
    x.current += 1;
    return c;
}

overload assign(dest:LexerInput, lvalue src:LexerInput) {
    if (dest.file == src.file) {
        var destMax = max(dest.maxCurrent, dest.current);
        var srcMax = max(src.maxCurrent, src.current);
        dest.maxCurrent = max(destMax, srcMax);
        dest.current = src.current;
    }
    else {
        dest.file = src.file;
        dest.current = src.current;
        dest.maxCurrent = src.maxCurrent;
    }
}



//
// LexerError
//

record LexerError(
    file: SourceFile,
    where: SizeT
);

instance ClayError = LexerError;

overload displayError(e:LexerError) {
    errorWithLocation(e.file, e.where, "invalid token");
}

record LexerTabIndentationError(
    file: SourceFile,
    where: SizeT
);

instance ClayError = LexerTabIndentationError;

overload displayError(e:LexerTabIndentationError) {
    errorWithLocation(e.file, e.where, "tab characters are not allowed as indentation");
}


//
// tokenize
//

external lexer_tokenize(
    ex:ExternalException,
    inFile:Pointer[SourceFile],
    out:Pointer[SourceTokens]
);

tokenize(file) = callExternal(lexer_tokenize, file);
